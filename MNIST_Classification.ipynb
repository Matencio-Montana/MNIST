{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "576b12ab-c0c3-4445-946b-7d229952c5de",
   "metadata": {},
   "source": [
    "**LinkedIn Profile:** [Matencio Montana](https://www.linkedin.com/in/montana-matencio-b01111376)  \n",
    "**Contact:** [montana.matencio@gmail.com](mailto:montana.matencio@gmail.com)\n",
    "\n",
    "**MNIST Image Classification using CNN**\n",
    "\n",
    "This project focuses on building and evaluating a deep learning model for classifying handwritten digits from the MNIST dataset using a Convolutional Neural Network (CNN).\n",
    "\n",
    "**Project Goal:**\n",
    "The primary objective is to develop an accurate and efficient image classification model that can correctly identify handwritten digits (0-9).\n",
    "\n",
    "**Technologies & Methodologies:**\n",
    "\n",
    "- Python: The core programming language.\n",
    "\n",
    "- PyTorch: The deep learning framework used to build and train the CNN.\n",
    "\n",
    "- TorchVision: Utilized for easily accessing and transforming the MNIST dataset.\n",
    "\n",
    "- Convolutional Neural Networks (CNN): A specialized type of neural network highly effective for image processing tasks, employed here for feature extraction and classification.\n",
    "\n",
    "    - nn.Conv2d: For applying convolutional filters to input images.\n",
    "\n",
    "    - nn.BatchNorm2d: Implemented for normalizing layer outputs, improving training stability and convergence.\n",
    "\n",
    "    -nn.ReLU: The activation function used to introduce non-linearity into the model.\n",
    "\n",
    "    -nn.MaxPool2d: For downsampling feature maps, reducing computational cost and preventing overfitting.\n",
    "\n",
    "    -nn.Flatten: To convert the 2D feature maps into a 1D vector for the fully connected layer.\n",
    "\n",
    "    -nn.Linear: The final fully connected layer for classification.\n",
    "\n",
    "    -nn.Dropout: Implemented for regularization to prevent overfitting by randomly setting a fraction of input units to zero at each update during training.\n",
    "\n",
    "-nn.CrossEntropyLoss: The loss function used for multi-class classification problems.\n",
    "\n",
    "-torch.optim.AdamW: An adaptive optimization algorithm used to update model weights, with weight decay for regularization.\n",
    "\n",
    "-lr_scheduler.CosineAnnealingLR: A learning rate scheduler that adjusts the learning rate using a cosine annealing schedule, aiding in convergence and potentially finding better local minima.\n",
    "\n",
    "-NumPy & Random: Used for setting random seeds to ensure reproducibility.\n",
    "\n",
    "-GPU Acceleration: Leveraged torch.cuda when available for faster model training.\n",
    "\n",
    "**Data Source:**\n",
    "\n",
    "The model is trained and tested on the MNIST dataset, a widely used benchmark dataset of handwritten digits, readily available through torchvision.datasets.\n",
    "\n",
    "**Notebook Structure:**\n",
    "\n",
    "- Environment Setup: Setting random seeds for reproducibility and configuring device (CPU/GPU).\n",
    "\n",
    "- Data Loading & Preprocessing: Loading the MNIST dataset and applying ToTensor() transformation.\n",
    "\n",
    "- DataLoader Setup: Creating data loaders for efficient batch processing of training and testing data.\n",
    "\n",
    "- Model Definition: Building the MNIST_CNN architecture with convolutional, batch normalization, activation, pooling, dropout, flatten, and linear layers.\n",
    "\n",
    "- Hyperparameter Initialization: Setting up input_channels, hidden_channels, output_size, images_size, and dropout_prob.\n",
    "\n",
    "- Loss Function and Optimizer: Defining nn.CrossEntropyLoss and torch.optim.AdamW.\n",
    "\n",
    "- Training Loop: Implementing the training_loop function for iterating through batches, performing forward pass, calculating loss, backpropagation, and weight updates.\n",
    "\n",
    "- Testing Loop: Implementing the test_loop function for evaluating model performance (accuracy and average loss) on the test set.\n",
    "\n",
    "- Learning Rate Scheduling: Utilizing lr_scheduler.CosineAnnealingLR to dynamically adjust the learning rate during training.\n",
    "\n",
    "- Model Training & Evaluation: Executing the training and testing loops over several epochs, displaying progress and performance metrics.\n",
    "\n",
    "**Reproducibility:**\n",
    "- Random seeds are set at the beginning of the script to ensure the reproducibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d484bb-ab08-46df-99af-205d3da67da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9003c4-8ca5-4eed-818f-cbd08f7ef8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42 for all relevant libraries.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os # To set environment variables, useful for some libraries\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed for reproducibility across different libraries.\n",
    "    \"\"\"\n",
    "    # 1. Set seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # 2. Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 3. Set seed for PyTorch (CPU and GPU)\n",
    "    torch.manual_seed(seed) # For CPU operations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed) # For current GPU\n",
    "        torch.cuda.manual_seed_all(seed) # For all GPUs (if you have multiple)\n",
    "\n",
    "    # 4. Ensure deterministic behavior for CuDNN (GPU operations)\n",
    "    #    This can sometimes slightly slow down training, but ensures exact reproducibility.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # Disable CuDNN auto-tuner for deterministic ops\n",
    "\n",
    "    # 5. Set environment variable for Python hashing (affects dicts, sets, etc.)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    print(f\"Random seed set to {seed} for all relevant libraries.\")\n",
    "\n",
    "MY_RANDOM_SEED = 42\n",
    "set_seed(MY_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea04d974-9ece-4093-b335-224372d969ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "243679ef-bcf7-49c6-9e6e-363620c3c1ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc53a8d-4d1d-4457-8d45-ff54f5195dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "training_dataloader = DataLoader(training_data, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "for X,y in training_dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e09beb-7116-4fb9-8847-ed8ef8e34c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_size, images_size, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(input_channels,hidden_channels, kernel_size=(3,3), stride=(1,1), padding=(1,1) ),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_channels * images_size[0]//2 * images_size[1]//2, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.cnn(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d10934-0e41-40c3-b4d6-9303ac2edd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 1\n",
    "hidden_channels = 32\n",
    "output_size = 10\n",
    "images_size = (28,28)\n",
    "dropout_prob = 0.3\n",
    "\n",
    "model_CNN = MNIST_CNN(input_channels, hidden_channels, output_size, images_size, dropout_prob).to(device)\n",
    "criterion_CNN = nn.CrossEntropyLoss() # We use CrossEntropyLoss as we are solving a classification problem\n",
    "optimizer_CNN = torch.optim.AdamW(model_CNN.parameters(), lr=0.0001, weight_decay=0.0001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254446ca-7373-4adb-a16a-5dd415a66785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataloader,model,criterion, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_samples_processed_in_epoch = 0 #Initialize a variable to track the total samples processed in this epoch\n",
    "    model.train()\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred=model(X)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() #update of weights and biases  \n",
    "        optimizer.zero_grad() #gradient reset \n",
    "\n",
    "        #Accumulate the number of samples processed in the current batch\n",
    "        total_samples_processed_in_epoch += len(X) \n",
    "\n",
    "\n",
    "        if batch%100==0:\n",
    "            loss_val = loss.item()\n",
    "            print(f\"loss: {loss_val:>7f}  [{total_samples_processed_in_epoch:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    elements_per_batch = len(dataloader)\n",
    "    model.eval()\n",
    "    sum_loss_per_batch, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            sum_loss_per_batch+=criterion(pred,y).item()\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    sum_loss_per_batch/=elements_per_batch\n",
    "    correct/=size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {sum_loss_per_batch:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "671e8e17-35e7-441a-9ee1-25920ebe905d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.373308  [  128/60000]\n",
      "loss: 0.433661  [12928/60000]\n",
      "loss: 0.342856  [25728/60000]\n",
      "loss: 0.229722  [38528/60000]\n",
      "loss: 0.287554  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.206569 \n",
      "\n",
      "Current Learning Rate: 0.000091\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.232835  [  128/60000]\n",
      "loss: 0.411702  [12928/60000]\n",
      "loss: 0.232680  [25728/60000]\n",
      "loss: 0.200513  [38528/60000]\n",
      "loss: 0.082251  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.145730 \n",
      "\n",
      "Current Learning Rate: 0.000069\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.129178  [  128/60000]\n",
      "loss: 0.100402  [12928/60000]\n",
      "loss: 0.132377  [25728/60000]\n",
      "loss: 0.097645  [38528/60000]\n",
      "loss: 0.123066  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.116954 \n",
      "\n",
      "Current Learning Rate: 0.000041\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.133229  [  128/60000]\n",
      "loss: 0.133255  [12928/60000]\n",
      "loss: 0.190191  [25728/60000]\n",
      "loss: 0.157921  [38528/60000]\n",
      "loss: 0.096495  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.107466 \n",
      "\n",
      "Current Learning Rate: 0.000019\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.128645  [  128/60000]\n",
      "loss: 0.096144  [12928/60000]\n",
      "loss: 0.097061  [25728/60000]\n",
      "loss: 0.105738  [38528/60000]\n",
      "loss: 0.042751  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.101632 \n",
      "\n",
      "Current Learning Rate: 0.000010\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "scheduler_CNN = lr_scheduler.CosineAnnealingLR(optimizer_CNN, T_max=epochs, eta_min=1e-5)\n",
    "for iteration in range(epochs):\n",
    "    print(f\"Epoch {iteration+1}\\n-------------------------------\")\n",
    "    training_loop(training_dataloader,model_CNN,criterion_CNN,optimizer_CNN)\n",
    "    test_loop(test_dataloader,model_CNN,criterion_CNN)\n",
    "    scheduler_CNN.step()\n",
    "    #display the current learning rate \n",
    "    current_lr = optimizer_CNN.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
